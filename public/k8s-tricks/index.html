<!doctype html><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="thoughts.theden.sh — Denis Khoshaba"><link rel="shortcut icon" href=https://thoughts.theden.sh/favicon.ico><link rel=stylesheet href=/css/style.min.css><title>Useful Tricks & Lessons I've learned Managing a Kubernetes Cluster</title><header id=banner><h2><a href=https://thoughts.theden.sh/>thoughts</a></h2><nav><ul><li><a href=/ title=posts>posts</a></ul></nav></header><main id=content><article><header id=post-header><h1>Useful Tricks & Lessons I've learned Managing a Kubernetes Cluster</h1><div><time>May 2, 2020</time></div></header><h1 id=debugging-pods>Debugging Pods</h1><p>You can use <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/#ephemeral-container>ephemeral debug containers</a> but as of writing they&rsquo;re a feature state for <code>Kubernetes v1.18 [alpha]</code>.<p>What I use is <code>kubectl edit</code> to modify a pod (or <code>Deployment</code>, <code>StatefulSet</code> etc.) YAML to update the <code>command</code><div class=highlight><pre class=chroma><code class=language-YAML data-lang=YAML><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>unstable-pod</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>unstable-pod</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>foobar</span><span class=w>
</span><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=l>sh</span><span class=w>
</span><span class=w>    </span>- -<span class=l>c</span><span class=w>
</span><span class=w>    </span>- <span class=s2>&#34;tail -f /dev/null&#34;</span><span class=w>
</span></code></pre></div><p>From there I can <code>kubectl exec</code> into the pod and manually start the entrypoint. Useful when you want to keep the pod running when main PIDs crash or if you want to play around with configs quickly without editing YAML configs and restarting the pod.<h1 id=using-podmanagementpolicyhttpskubernetesiodocstutorialsstateful-applicationbasic-stateful-setpod-management-policy>Using <a href=https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy>podManagementPolicy</a></h1><p>Mostly used for StatefulSets. Setting <code>podManagementPolicy: "Parallel"</code> means the ordered pods will run and be terminated in parallel. Useful for when you don&rsquo;t care about pods starting serially in order. Keep in mind, if you scale a broken deployment with <code>n</code> pods, you&rsquo;ll have <code>n</code> <code>CrashLoopBackOffs</code>.<h1 id=defaulting-to-retain-pvs>Defaulting to Retain PVs</h1><p>For all our PVs, we create a <code>StorageClass</code> with something like<div class=highlight><pre class=chroma><code class=language-YAML data-lang=YAML><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>storage.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>StorageClass</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>standard-pd-retain</span><span class=w>
</span><span class=w></span><span class=nt>provisioner</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes.io/gce-pd</span><span class=w>
</span><span class=w></span><span class=nt>reclaimPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Retain</span><span class=w>
</span><span class=w></span><span class=nt>allowVolumeExpansion</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w></span><span class=nt>volumeBindingMode</span><span class=p>:</span><span class=w> </span><span class=l>WaitForFirstConsumer</span><span class=w>
</span><span class=w></span><span class=nt>parameters</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>pd-standard</span><span class=w>
</span></code></pre></div><p>Where <code>reclaimPolicy: Retain</code> ensures if PVs are deleted, on the storage backed side (EBS, GCP Disks etc.) are preserved. The general idea is not to let Kubernetes actually delete any data, the PV will disappear but the volume will still live. This is useful when you accidentally delete a PV, since it can be reattached to a new PVC for recovery (I&rsquo;ve had to do this).<h1 id=deep-dive-through-abstractions>Deep-dive Through Abstractions</h1><p>When something goes awry, and you probably don&rsquo;t have any sophisticated monitoring because you may not have the time to build it yet, and if the issue urgent you&rsquo;ll probably need to get familiar with checking the right places. Lets say for example, if something is broken, and it&rsquo;s not immediately obvious if it&rsquo;s an application or a cluster-wide issue, the order I&rsquo;d look into things are:<ul><li>Go by my first hunch and follow that (as we&rsquo;re naturally inclined to do). After a while you learn the general pitfalls, or have a feeling it may be due to a recent change.<li>Starting by <code>kubectl describe</code> of the pods that have issues and check the events<li><code>kubectl get events</code> in the namespace. Then <code>kubectl get events --sort-by=.metadata.creationTimestamp</code><li><code>kubectl get pods -o wide</code> to see the node the pod is running on. Then <code>kubectl describe</code> the node to see if there are issues with it. Might as well check the other nodes just in case<li>If it&rsquo;s an issue with PVCs, check the persistent volumes, sometimes they get lost, <code>kubectl get pv</code> or <code>kubectl describe pv</code>. If PVs are stuck in <code>Terminating</code>, edit the PV and <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/#storage-object-in-use-protection>delete the <code>Finalizers</code></a><li>If it&rsquo;s a performance issue, use <code>watch -n5 kubectl top nodes</code> (or <code>kubectl top pods</code>)<li>If it&rsquo;s a scaling issue check the autoscaler <code>kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler</code><li>If it&rsquo;s a networking issue check the Kubernetes DNS. We&rsquo;ve ran into <a href=https://github.com/kubernetes/kubernetes/issues/41125>this issue</a>, where all the <code>kube-dns</code> pods were scheduled on one preemptible node, guaranteeing DNS downtime<li>Check the VMs on your cloud provider if it&rsquo;s compute issues, or the disk if it&rsquo;s storage. Sometimes it&rsquo;s preemptible/spot nodes just being down (or the general provider itself)</ul><p>Usually the problem will emerge. I&rsquo;ve yet had had to connect to the nodes themselves to debug issues.<h1 id=ditching-statefulsets>Ditching StatefulSets</h1><p>StatefulSets have their place, but in my opinion they&rsquo;re not ideal when you really want to keep state. Useful for say general compute pods where it&rsquo;s useful to keep state between pod restarts and the like. But say you&rsquo;re running a properly stateful service like HDFS or ES on Kubernetes. A <code>Deployment</code> + <code>PVC</code> is <em>always</em> better. Here are some comparisons, say for a HDFS (namenode) pod with <code>n</code> datanodes:<ul><li>If you want to remove datanode number <code>4</code> of <code>n</code> in a StatefulSet, you can&rsquo;t, you have to scale them down since they&rsquo;re ordered<li>If you want to migrate the PVs to a different cluster, I honestly wouldn&rsquo;t know how to, since the <code>volumeClaimTemplates</code> will create new PVCs + PVs<li>If you accidentally delete the StatefulSet you lose the <code>volumeClaimTemplates</code>, and I wouldn&rsquo;t know how to reattach the old PVs if they&rsquo;re set to <code>Retain</code>.<li>You can&rsquo;t have datanodes in a StatefulSet have different PVC sizes, <code>requests</code>, env vars, or anything useful. They stay identical. Say you need to resize a few datanode disks to save costs, or run the last two of the pods with fewer resource and you need to do it—I can&rsquo;t think of any easy way to do this.</ul><p>If you require any of these I would recommend using a <code>Deployment</code> + <code>PVC</code> + <code>PV</code>. There&rsquo;s some work involved in scaling, since you&rsquo;re basically creating your own &ldquo;StatefulSet&rdquo;, but you have more control of each deployment, and it will save a lot of headache if you run into any issues. I may be wrong here, if anyone has had success with solving the above problems with StatefulSets I&rsquo;d like to know about them.<h1 id=using-kubectl-diff-and-validate---dry-run>Using <code>kubectl diff</code> and <code>validate --dry-run</code></h1><p>Both useful for your CI of course, but generally a good idea to check diff of a resource you&rsquo;re about to update. I&rsquo;ve been using it as often as I use <code>git diff</code>. Just make sure you use a client post <code>1.18.1</code> due to <a href=https://github.com/kubernetes/kubectl/issues/852>this issue</a> where a bug causes <code>diff</code> to <code>apply</code><h1 id=switching-between-clusters-and-namespaces-quickly>Switching Between Clusters and Namespaces Quickly</h1><p>I personally use <a href=https://github.com/ahmetb/kubectx>kubectx + kubens</a> to easily switch between different clusters and namespaces. I also renames both binaries to <code>kcontext</code> and <code>knamespace</code> since I don&rsquo;t want to ruin my <code>kubectl</code> autocomplete.<h1 id=keeping-it-simple>Keeping it Simple</h1><p>I use <code>kustomize</code> + <code>envsubst</code> + <code>python3</code> to template via overlays. I&rsquo;m not into templating YAML (kustomize is advertised as &ldquo;template-free&rdquo; templating, I guess it is but that comes with limitations and it&rsquo;s own <code>YAML HELL</code>). I don&rsquo;t use helm (more moving parts, black box, &ldquo;package management&rdquo; aspect to it isn&rsquo;t a feature for me etc.), or try not to i.e., either build my own YAML or generate on from the chart. I try not use use Kubernetes Operators or too many CRDs etc., generally keeping things stock. Of course complexity is always difficult to manage with real deadlines.<p>I would probably approach YAML creation programatically (especially if the Kubernetes use-case is SaaS-like), however being one person managing clusters, I couldn&rsquo;t imagine getting things shipped—but there may be other opportunities to do some elegant work here in the future.<p>Probably the most important thing I&rsquo;ve learned is the hardest part of managing a Kubernetes cluster is pushing back on the complexity. It&rsquo;s easy run with it, to the point where the cluster complexity-space is larger than people can manage. I&rsquo;m also starting to think over-engineering is a real problem, and with time systems tend to converge as technical debt. Some of these are hard to predict or know. After 3 years managing a single Kubernetes cluster, what we considered a great idea or &ldquo;best practice&rdquo; two years ago now may seem archaic. Sometimes you internalise the complexity and don&rsquo;t realise the knowledge your team or others have in the system—of course, documentation solves many problems, but I&rsquo;m still always thinking of reducing complexity.</article></main><footer id=footer></footer>