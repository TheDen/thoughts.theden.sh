<!doctype html><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="thoughts.theden.sh — Denis Khoshaba"><link rel="shortcut icon" href=public/favicon.ico><link rel=stylesheet href=/public/css/hack.min.css><title>Kube Tricks</title><header id=banner><h2><a href=public/>thoughts.theden.sh</a></h2><nav><ul></ul></nav></header><main id=content><article><header id=post-header><h1>Kube Tricks</h1><div><time>November 10, 2021</time></div></header><h2 id=ephemeral-debug-containers>Ephemeral Debug Containers</h2><p>One can use <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/#ephemeral-container>ephemeral debug containers</a><p>Alternatively to edit in-place to test out configs and env vars use <code>kubectl edit</code> to modify a pod (or <code>Deployment</code>, <code>StatefulSet</code> etc.) YAML to update the command to do nothing so one can <code>kubectl exec</code> into the pod<div class=highlight><pre tabindex=0 class=chroma><code class=language-YAML data-lang=YAML><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>unstable-pod</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>unstable-pod</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>foobar</span><span class=w>
</span><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=l>sh</span><span class=w>
</span><span class=w>    </span>- -<span class=l>c</span><span class=w>
</span><span class=w>    </span>- <span class=s2>&#34;tail -f /dev/null&#34;</span><span class=w>
</span></code></pre></div><p>Ensure <code>liveness</code> and <code>readiness</code> probes are commented out if it depends on a runtime condition.<h2 id=utilising-podmanagementpolicyhttpskubernetesiodocstutorialsstateful-applicationbasic-stateful-setpod-management-policy>Utilising <a href=https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy>podmanagementpolicy</a></h2><p>Mostly used for <code>StatefulSets</code>. Setting <code>podManagementPolicy: "Parallel"</code> means the ordered pods will run and be terminated in parallel. Useful for when one doesn&rsquo;t care about pods starting serially, in order. Keep in mind, if a broken deployment is scaled with <code>n</code> pods, there will be an <code>n</code> amount of <code>CrashLoopBackOffs</code> in parallel.<h2 id=defaulting-to-retain-pvcs>Defaulting to Retain <code>PVCs</code></h2><p>For example, creating a <code>StorageClass</code> (e.g., if GCP is the provider)<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>storage.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>StorageClass</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>standard-pd-retain</span><span class=w>
</span><span class=w></span><span class=nt>provisioner</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes.io/gce-pd</span><span class=w>
</span><span class=w></span><span class=nt>reclaimPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Retain</span><span class=w>
</span><span class=w></span><span class=nt>allowVolumeExpansion</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w></span><span class=nt>volumeBindingMode</span><span class=p>:</span><span class=w> </span><span class=l>WaitForFirstConsumer</span><span class=w>
</span><span class=w></span><span class=nt>parameters</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>pd-standard</span><span class=w>
</span></code></pre></div><p>Where <code>reclaimPolicy: Retain</code> ensures if <code>PVs</code> are deleted, on the storage backed side (EBS, GCP Disks etc.) are preserved. The general idea is not to let Kubernetes actually delete any data, the <code>PV</code> will disappear but the volume will still persist outside of Kubernetes. This is useful when one accidentally deletes a <code>PV</code>, since it can be reattached to a new <code>PVC</code> for recovery (I’ve had to do this unfortunately).<h2 id=debugging-methods>Debugging Methods</h2><p>When something goes awry, and sometimes there isn&rsquo;t sophisticated monitoring already set up because there isn&rsquo;t enough time to build it yet—so one will probably need to get familiar with checking the right places, especially if an issue is urgent.<p>For example if something is broken and it’s not immediately obvious if it’s an application or a cluster-wide issue, a good order to deep-dive into things would be:<ul><li>Go by the first hunch and follow that (as one is naturally inclined to do). After a while one will learn the general pitfalls, or have built intuition.<li>Start by running <code>kubectl describe</code> on the pods that have issues and check the <code>events</code>.<li>Run <code>kubectl get events</code> in the namespace. Then <code>kubectl get events --sort-by=.metadata.creationTimestamp</code> and inspect.<li>Check <code>kubectl get pods -o wide</code> to see which node the pod is running on. Then <code>kubectl describe nodes</code> to see if there are issues with the particular node (or any others).<li>If it’s an issue with <code>PVCs</code>, check the persistent volumes since they sometimes get lost, via <code>kubectl get pv</code> or <code>kubectl describe pv</code>. If <code>PVs</code> are stuck as <code>Terminating</code>, edit the <code>PV</code> and delete the <code>Finalizers</code>.<li>If it’s a performance issue, use <code>watch -n5 kubectl top nodes</code> (or <code>kubectl top pods</code>) and inspect.<li>If it’s a scaling issue check <code>autoscaler kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler</code>.<li>If it’s a networking issue check the Kubernetes DNS. In certain infra configurations <code>kube-dns</code> pods can be scheduled on only one node (sometimes a preemptible or spot node), guaranteeing downtime.<li>Check the VMs on the cloud provider if it&rsquo;s a compute issues, or the disk if it’s a storage issue. Sometimes it&rsquo;s preemptible/spot nodes going down (or the general provider itself)
Usually the problem will emerge. Normally one doesn&rsquo;t often need to directly connect to the VM.</ul><h2 id=statefulsets-are-more-trouble-than-theyre-worth><code>StatefulSets</code> are more trouble than they&rsquo;re worth</h2><p><code>StatefulSets</code> have their place, but in my opinion they’re not ideal when one really wants to principally keep state. Useful for general compute pods where it’s good to keep state between pod restarts and the like. But in situations where on is running a properly stateful service like <code>HDFS</code> or <code>ES</code>, a <code>Deployment</code> + <code>PVC</code> is almost always better. Here are some comparisons, say for a <code>HDFS</code> (namenode) pod with <code>n</code> datanodes:<ul><li>If one wants to remove ; number 4 of n in a <code>StatefulSet</code>, one has to scale them down since they’re ordered<li>If one wants to migrate the <code>PVs</code> to a different cluster, it&rsquo;s non-trivial since <code>volumeClaimTemplates</code> will create new <code>PVCs</code> + <code>PVs</code><li>If one accidentally delete the <code>StatefulSet</code>, the <code>volumeClaimTemplates</code> is removed, and non-trivial to reattach <code>PVs</code> (if they were set to <code>Retain</code>).<li>One can&rsquo;t have datanodes in a <code>StatefulSet</code> that have different <code>PVC</code> sizes, requests, env vars, or anything useful. They stay identical.</ul><h2 id=using-kubectl-diff-and-validate---dry-run>Using <code>kubectl diff</code> and <code>validate --dry-run</code></h2><p>Both useful for CI, but generally a good idea to check the <code>diff</code> of a resource that&rsquo;s about to updated.<h2 id=switch-namespaces-and-clusters-faster>Switch Namespaces and Clusters Faster</h2><p>Tools like <a href=https://github.com/ahmetb/kubectx>kubectx + kubens</a> are useful to switch between different clusters and namespaces faster. Renaming both binaries to <code>kcontext</code> and <code>knamespace</code> is another neat trick to make <code>kubectl</code> autocomplete.<h2 id=keeping-it-simple>Keeping it Simple</h2><p>Personally, I prefer <code>kustomize</code> + <code>envsubst</code> + <code>python</code> to template via <code>overlays</code>. I’m not into templating YAML (kustomize is advertised as &ldquo;template-free&rdquo; templating, but it&rsquo;s still somewhat <code>YAML HELL</code> like many things).<p>Personally I avoid using helm (messy configs, more moving parts, black box, the &ldquo;package management&rdquo; isn&rsquo;t a feature for me for infra code), or try not to i.e., generate <code>YAMLs</code> from a chart and use them. I try not use use too many Kubernetes operators or <code>CRDs</code>, generally keeping things stock and primitive. Of course complexity is always difficult to manage, so compromises are made. Programatically generating Kubernetes manifests is a good engineering idea, but probably overkill for many teams.</article></main><footer id=footer><a class=footer-link href=https://theden.sh>theden.sh</a></footer>